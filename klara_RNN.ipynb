{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of load_steinmetz_decisions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/klaragerlei/GoatsOfAllTime/blob/main/klara_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEqdz1ZUMaj1"
      },
      "source": [
        "## Loading of Steinmetz data\n",
        "\n",
        "includes some visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLWjKq8bLDqm",
        "cellView": "form"
      },
      "source": [
        "#@title Data retrieval\n",
        "import os, requests\n",
        "\n",
        "fname = []\n",
        "for j in range(3):\n",
        "  fname.append('steinmetz_part%d.npz'%j)\n",
        "url = [\"https://osf.io/agvxh/download\"]\n",
        "url.append(\"https://osf.io/uv3mw/download\")\n",
        "url.append(\"https://osf.io/ehmw2/download\")\n",
        "\n",
        "for j in range(len(url)):\n",
        "  if not os.path.isfile(fname[j]):\n",
        "    try:\n",
        "      r = requests.get(url[j])\n",
        "    except requests.ConnectionError:\n",
        "      print(\"!!! Failed to download data !!!\")\n",
        "    else:\n",
        "      if r.status_code != requests.codes.ok:\n",
        "        print(\"!!! Failed to download data !!!\")\n",
        "      else:\n",
        "        with open(fname[j], \"wb\") as fid:\n",
        "          fid.write(r.content)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raBVOEWgUK_B",
        "cellView": "form"
      },
      "source": [
        "#@title Import matplotlib and set defaults\n",
        "from matplotlib import rcParams \n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "rcParams['figure.figsize'] = [20, 4]\n",
        "rcParams['font.size'] =15\n",
        "rcParams['axes.spines.top'] = False\n",
        "rcParams['axes.spines.right'] = False\n",
        "rcParams['figure.autolayout'] = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sffzC_hyLgWZ"
      },
      "source": [
        "#@title Data loading\n",
        "import numpy as np\n",
        "alldat = np.array([])\n",
        "for j in range(len(fname)):\n",
        "  alldat = np.hstack((alldat, np.load('steinmetz_part%d.npz'%j, allow_pickle=True)['dat']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5K7UT7dyj_6R"
      },
      "source": [
        "`alldat` contains 39 sessions from 10 mice, data from Steinmetz et al, 2019. Time bins for all measurements are 10ms, starting 500ms before stimulus onset. The mouse had to determine which side has the highest contrast. For each `dat = alldat[k]`, you have the fields below. For extra variables, check out the extra notebook and extra data files (lfp, waveforms and exact spike times, non-binned). \n",
        "\n",
        "* `dat['mouse_name']`: mouse name\n",
        "* `dat['date_exp']`: when a session was performed\n",
        "* `dat['spks']`: neurons by trials by time bins.    \n",
        "* `dat['brain_area']`: brain area for each neuron recorded. \n",
        "* `dat['ccf']`: Allen Institute brain atlas coordinates for each neuron. \n",
        "* `dat['ccf_axes']`: axes names for the Allen CCF. \n",
        "* `dat['contrast_right']`: contrast level for the right stimulus, which is always contralateral to the recorded brain areas.\n",
        "* `dat['contrast_left']`: contrast level for left stimulus. \n",
        "* `dat['gocue']`: when the go cue sound was played. \n",
        "* `dat['response_time']`: when the response was registered, which has to be after the go cue. The mouse can turn the wheel before the go cue (and nearly always does!), but the stimulus on the screen won't move before the go cue.  \n",
        "* `dat['response']`: which side the response was (`-1`, `0`, `1`). When the right-side stimulus had higher contrast, the correct choice was `-1`. `0` is a no go response. \n",
        "* `dat['feedback_time']`: when feedback was provided. \n",
        "* `dat['feedback_type']`: if the feedback was positive (`+1`, reward) or negative (`-1`, white noise burst).  \n",
        "* `dat['wheel']`: turning speed of the wheel that the mice uses to make a response, sampled at `10ms`. \n",
        "* `dat['pupil']`: pupil area  (noisy, because pupil is very small) + pupil horizontal and vertical position.\n",
        "* `dat['face']`: average face motion energy from a video camera. \n",
        "* `dat['licks']`: lick detections, 0 or 1.   \n",
        "* `dat['trough_to_peak']`: measures the width of the action potential waveform for each neuron. Widths `<=10` samples are \"putative fast spiking neurons\". \n",
        "* `dat['%X%_passive']`: same as above for `X` = {`spks`, `pupil`, `wheel`, `contrast_left`, `contrast_right`} but for  passive trials at the end of the recording when the mouse was no longer engaged and stopped making responses. \n",
        "* `dat['prev_reward']`: time of the feedback (reward/white noise) on the previous trial in relation to the current stimulus time. \n",
        "* `dat['reaction_time']`: ntrials by 2. First column: reaction time computed from the wheel movement as the first sample above `5` ticks/10ms bin. Second column: direction of the wheel movement (`0` = no move detected).  \n",
        "\n",
        "\n",
        "The original dataset is here: https://figshare.com/articles/dataset/Dataset_from_Steinmetz_et_al_2019/9598406"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpE8gVVt38DI"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def assign_numbers_to_brain_areas(recording, brain_groups):\n",
        "  # print('Areas in this recording:' + str(np.unique(recording['brain_area'])))\n",
        "  nareas = len(brain_groups)\n",
        "  NN = len(recording['brain_area']) # number of neurons\n",
        "  barea = nareas * np.ones(NN, ) # last one is \"other\"\n",
        "  for area_id in range(nareas):\n",
        "    barea[np.isin(recording['brain_area'], brain_groups[area_id])] = area_id # assign a number to each region\n",
        "  return barea\n",
        "\n",
        "\n",
        "def get_firing_data_for_area(df_to_add_to, data, name_of_area, brain_groups):\n",
        "  index_corresponding_to_area = np.where(brain_groups == name_of_area)[0][0]\n",
        "  # print(index_corresponding_to_area)\n",
        "  spikes = []\n",
        "  for recording_id in range(len(data)):\n",
        "    recording = data[recording_id]\n",
        "    barea = assign_numbers_to_brain_areas(recording, brain_groups)\n",
        "    spikes_in_recording = recording['spks'][barea==index_corresponding_to_area]\n",
        "    # print(spikes_in_recording)\n",
        "    spikes.append(spikes_in_recording)\n",
        "  df_to_add_to['spikes'] = spikes\n",
        "  return df_to_add_to\n",
        "\n",
        "\n",
        "def load_behavioural_feature(df_to_add_to, data, column_name):\n",
        "  features = []\n",
        "  for recording_id in range(len(data)):  \n",
        "    recording_data = data[recording_id]  # this is just one recording\n",
        "    feature = recording_data[column_name]\n",
        "    features.append(feature)\n",
        "  df_to_add_to[column_name] = features\n",
        "  return df_to_add_to\n",
        "\n",
        "def drop_sessions_with_no_spikes(data_to_analyze):\n",
        "  sessions_to_drop = []\n",
        "  for session_index, session in data_to_analyze.iterrows():\n",
        "    if len(session.spikes) == 0:\n",
        "      sessions_to_drop.append(session_index)\n",
        "  \n",
        "  data_to_analyze = data_to_analyze.drop(sessions_to_drop, axis=0)\n",
        "  return data_to_analyze\n",
        "\n",
        "\n",
        "# Load firing data and behavioural variable for a given area\n",
        "def load_data_for_model(brain_area, behavioural_feature, brain_groups):\n",
        "  data_to_analyze = pd.DataFrame()  # make empty df\n",
        "  # add behavioural feature to df\n",
        "  data_to_analyze = load_behavioural_feature(data_to_analyze, alldat, behavioural_feature)\n",
        "  # add spikes to df\n",
        "  data_to_analyze = get_firing_data_for_area(data_to_analyze, alldat, brain_area, brain_groups)\n",
        "  return data_to_analyze\n",
        "\n",
        "\n",
        "def load_data(alldat, brain_area='MOp', feature='face'):\n",
        "  brain_groups = np.array([\"VISa\", \"VISam\", \"VISl\", \"VISp\", \"VISpm\", \"VISrl\",\"CL\", \"LD\", \"LGd\", \"LH\", \"LP\", \"MD\", \"MG\", \"PO\", \"POL\", \"PT\", \"RT\", \"SPF\", \"TH\", \"VAL\", \"VPL\", \"VPM\",\"CA\", \"CA1\", \"CA2\", \"CA3\", \"DG\", \"SUB\", \"POST\",\"ACA\", \"AUD\", \"COA\", \"DP\", \"ILA\", \"MOp\", \"MOs\", \"OLF\", \"ORB\", \"ORBm\", \"PIR\", \"PL\", \"SSp\", \"SSs\", \"RSP\",\"TT\",\"APN\", \"IC\", \"MB\", \"MRN\", \"NB\", \"PAG\", \"RN\", \"SCs\", \"SCm\", \"SCig\", \"SCsg\", \"ZI\",\"ACB\", \"CP\", \"GPe\", \"LS\", \"LSc\", \"LSr\", \"MS\", \"OT\", \"SNr\", \"SI\",\"BLA\", \"BMA\", \"EP\", \"EPd\", \"MEA\"])\n",
        "  data_to_analyze = load_data_for_model(brain_area=brain_area, behavioural_feature=feature, brain_groups=brain_groups)\n",
        "  # data_to_analyze = load_data_for_model(brain_area=brain_area, behavioural_feature=behavioural_feature, brain_groups=brain_groups)\n",
        "  data_to_analyze = drop_sessions_with_no_spikes(data_to_analyze)\n",
        "  # print(data_to_analyze.head())\n",
        "  return data_to_analyze\n",
        "\n",
        "\n",
        "data_to_analyze = load_data(alldat)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR0OVagbSC8O"
      },
      "source": [
        "def change_bin_size(array_in, window_size=10):\n",
        "  array_with_different_bins = np.add.reduceat(array_in, range(0, len(array_in), window_size))\n",
        "  return array_with_different_bins\n",
        "\n",
        "#test_array = np.array([0, 1, 0, 0, 1, 0,3, 4, 5, 4, 1, 2, 3, 4, 5, 5, 0])\n",
        "#print(test_array.shape)\n",
        "#array_out = change_bin_size(test_array, window_size=10)\n",
        "#print(test_array)\n",
        "#print(array_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pp_WwQMBWgmp",
        "outputId": "7184f885-c94d-4070-958e-1a5bcd63b81e"
      },
      "source": [
        "def convert_data_to_bigger_bin_size_spikes(data_to_analyze):\n",
        "  spikes_big_bins = []\n",
        "  for session_id, session in data_to_analyze.iterrows():\n",
        "    spikes_in_session = []\n",
        "    spikes = session.spikes\n",
        "    for neuron in range(spikes.shape[0]):\n",
        "      spikes_from_neuron = []\n",
        "      spikes_neuron = spikes[neuron]\n",
        "      spikes_all_trials = np.array(spikes_neuron.reshape(-1))\n",
        "      spikes_new_bin_size = change_bin_size(spikes_all_trials, window_size=10)\n",
        "      spikes_in_session.append(spikes_new_bin_size)\n",
        "    spikes_big_bins.append(spikes_in_session)\n",
        "  data_to_analyze['spikes_bigger_bins'] = spikes_big_bins\n",
        "  print(data_to_analyze.spikes_bigger_bins)\n",
        "  return data_to_analyze\n",
        "\n",
        "data_to_analyze = convert_data_to_bigger_bin_size_spikes(data_to_analyze)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10    [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
            "20    [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,...\n",
            "28    [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
            "Name: spikes_bigger_bins, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kw5nxqbLqE0k"
      },
      "source": [
        "def convert_data_to_bigger_bin_size_face(data_to_analyze):\n",
        "  face_data = []\n",
        "  for session_id, session in data_to_analyze.iterrows():\n",
        "    face_in_session = session.face\n",
        "    face_data_flat = face_in_session.reshape(-1)\n",
        "    face_new_bin_size = change_bin_size(face_data_flat, window_size=10)\n",
        "    face_data.append(face_new_bin_size)\n",
        "  data_to_analyze['face_bigger_bins'] = face_data\n",
        "  return data_to_analyze\n",
        "\n",
        "data_to_analyze = convert_data_to_bigger_bin_size_face(data_to_analyze)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbBZYK6rfPyK"
      },
      "source": [
        "def add_number_of_spikes_per_neuron_to_df(df):\n",
        "  number_of_spikes = []\n",
        "  for recording_index, recording in df.iterrows():\n",
        "    spikes_all = recording.spikes\n",
        "    spikes_neuron = []\n",
        "    for neuron in range(spikes_all.shape[0]):\n",
        "      num_of_spikes = np.sum(spikes_all[neuron])\n",
        "      spikes_neuron.append(num_of_spikes)\n",
        "    number_of_spikes.append(spikes_neuron)\n",
        "  df['number_of_spikes'] = number_of_spikes\n",
        "  return df      \n",
        "\n",
        "data_to_analyze = add_number_of_spikes_per_neuron_to_df(data_to_analyze)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HG1kS35j0ln"
      },
      "source": [
        "# plot spikes across time\n",
        "def plot_spikes_across_time():\n",
        "  session_id = 0\n",
        "  neuron_id = 10\n",
        "  # spikes_of_neuron = data_to_analyze.spikes[session_id][neuron_id]\n",
        "\n",
        "  session_to_analyze = data_to_analyze.spikes.iloc[session_id]\n",
        "  number_of_spikes = data_to_analyze.number_of_spikes.iloc[session_id][neuron_id]\n",
        "  print('Number of spikes: ' + str(number_of_spikes))\n",
        "  spikes_of_neuron = session_to_analyze.reshape(session_to_analyze.shape[0], -1)[neuron_id]\n",
        "  plt.figure()\n",
        "  plt.plot(spikes_of_neuron)\n",
        "  plt.show()\n",
        "  plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mgc-nMSulgPv"
      },
      "source": [
        "# plot pupil size\n",
        "def plot_pupil_size():\n",
        "  print(data_to_analyze.iloc[session_id].pupil.shape)\n",
        "  pupil_data_to_plot_1 = data_to_analyze.pupil.iloc[session_id][1].T[0]\n",
        "  pupil_data_to_plot_2 = data_to_analyze.pupil.iloc[session_id][2].T[0]\n",
        "  print(pupil_data_to_plot.shape)\n",
        "  plt.figure()\n",
        "  plt.plot(pupil_data_to_plot_1)\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "  plt.figure()\n",
        "  plt.plot(pupil_data_to_plot_2)\n",
        "  plt.show()\n",
        "  plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cv19hm-xO01o"
      },
      "source": [
        "def reshape_pupil_data():\n",
        "  # Make input features for model\n",
        "  session_id = 0   # we will analyze this session\n",
        "  session_to_analyze = data_to_analyze.spikes.iloc[session_id]\n",
        "  print('number of spikes')\n",
        "  print(data_to_analyze.number_of_spikes.iloc[session_id])\n",
        "  print('Number of neurons in this session: ' + str(session_to_analyze.shape[0]))\n",
        "  # try only one trial\n",
        "  trial_id = 2\n",
        "  session_to_analyze_reshaped = session_to_analyze[:,trial_id,:] # first trial \n",
        "  pupil_data = data_to_analyze.pupil.iloc[0][0,trial_id]  # first trial \n",
        "\n",
        "  print(session_to_analyze_reshaped.shape)\n",
        "  print(pupil_data.shape)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52NWZlr-P7vB",
        "outputId": "034e48e7-42f9-4f3e-e6d2-58b640950826"
      },
      "source": [
        "  def reshape_face_data(data_to_analyze, session_id=0, trial_id=None):\n",
        "    # Make input features for model\n",
        "    session_to_analyze = data_to_analyze.spikes.iloc[session_id]\n",
        "    print('number of spikes')\n",
        "    print(data_to_analyze.number_of_spikes.iloc[session_id])\n",
        "    print('Number of neurons in this session: ' + str(session_to_analyze.shape[0]))\n",
        "\n",
        "    session_to_analyze_reshaped = session_to_analyze[-1] # all trials \n",
        "    face_data = data_to_analyze.face.iloc[session_id][0,-1]  # all trials\n",
        "\n",
        "    if trial_id != None:\n",
        "      session_to_analyze_reshaped = session_to_analyze[:,trial_id,:] # one trial \n",
        "      face_data = data_to_analyze.face.iloc[session_id][0,trial_id]  # one trial \n",
        "\n",
        "\n",
        "    print(session_to_analyze_reshaped.shape)\n",
        "    print(face_data.shape)\n",
        "    return face_data, session_to_analyze_reshaped\n",
        "behaviour_data, session_to_analyze_reshaped = reshape_face_data(data_to_analyze, trial_id=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of spikes\n",
            "[4, 1784, 391, 8565, 204, 34, 4777, 301, 64, 5462, 0, 3077, 50, 51, 509, 2135, 141, 64, 5977, 5520, 484, 1822, 9, 79, 0, 287, 556, 41, 100, 14010, 114, 148, 174, 113, 1307, 5292, 147, 1849, 403, 691, 251, 5487, 93, 48, 2263, 59, 76, 100, 10227, 119, 454, 1676, 14, 253, 127, 1110, 1678, 1570, 130, 1812, 200, 117, 126, 7273, 129, 5138, 80, 301, 321, 24, 3637, 21, 5651, 4862, 28, 50, 723, 836, 1071, 50, 5002, 713, 35, 0, 187, 185, 2214, 11, 241, 1630, 2840, 275, 729, 2881, 185, 599, 136, 745, 14723, 217, 11110, 730, 267, 72, 130, 1636, 3669, 140, 1892, 118, 107, 41, 76, 8279, 895, 966, 68, 234, 80, 5509, 287, 6506, 7983, 303, 35, 137, 51, 41, 1265, 143, 180, 309, 1643, 287, 21338, 28892, 16645, 13582, 5876, 15118, 15296, 6715, 4707, 8655, 7542, 6413, 5596, 119, 10387, 5268, 2304, 1554, 1347, 3554, 9785, 140, 1719, 310, 23, 311, 607, 1657, 481, 17437, 1053, 43, 4900, 400, 1345, 6340, 44, 2809, 547, 440, 1845, 75, 1675, 2696, 1114, 321, 2708, 5707, 1953, 2710, 455, 873, 521, 248, 120, 4034, 101, 611, 330, 389, 1261, 1654, 379, 933, 616, 539, 174, 359, 284, 1106, 566, 929, 419, 107, 499, 752, 445, 314, 1482, 424, 1259, 2269, 508, 2189, 542, 1405, 61, 188, 449, 1077, 68, 937, 164, 570, 239, 844, 592, 3415, 389, 317, 468, 3150, 173, 955, 5999, 107, 199, 269, 89, 460, 286, 301, 1004, 957, 124, 492, 393, 22517, 920, 758, 1273, 401, 319, 11034, 7766, 11869, 1959, 8, 2380, 289, 0, 21207, 13857, 23154, 18301, 9967, 11768, 3590, 9186, 11604, 7814, 5830, 3930, 4727, 2168, 3190, 1608, 1483, 2669, 2635, 484, 15182, 716, 132, 319, 1697, 1188, 921, 738, 518, 177, 6269, 182, 593, 1475, 150, 2349, 15390, 1991, 81, 442, 1, 4, 5744, 1071, 5125, 2659, 1022, 2371, 2235, 4348, 1771, 520, 444, 672, 2, 8870, 1110, 69, 29221, 1253, 2752, 4344, 1295, 13626, 163, 5072, 350, 3538, 3424, 382, 203, 3714, 1052, 5337, 23, 190, 408, 470, 285, 620, 4732, 35, 326, 509, 11784, 2512, 6705, 146, 8, 3190, 4777, 37, 330, 69, 0, 2922, 993, 567, 570, 91, 711, 1707, 12, 850, 4743, 727, 1210, 1732, 389, 2792, 185, 2422, 3289, 2305, 93, 6, 59, 255, 8688, 7, 6692, 8638, 1620, 505, 1242, 490, 13897, 2033, 2015, 66, 1024, 719, 4069, 5635, 5221, 4043, 745, 1616, 3637, 5642, 83, 368, 5, 1439, 1064, 3, 111, 2698, 188, 1043, 1949, 238, 5372, 31, 2316, 1980, 55, 5089, 4706, 165, 1300, 846, 3232, 318, 12026, 5817, 23845, 17990, 31826, 57986, 19682, 17692, 16327, 11519, 10280, 18216, 13051, 5900, 5359, 8577, 7159, 7604]\n",
            "Number of neurons in this session: 447\n",
            "(342, 250)\n",
            "(250,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvuYenypojcc"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn2Azr-KpR56"
      },
      "source": [
        "Define network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ksH0Qlto3t3"
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self, ncomp, NN1, NN2, bidi=True):\n",
        "    super(Net, self).__init__()\n",
        "\n",
        "    # play with some of the options in the RNN!\n",
        "    self.rnn = nn.RNN(NN1, ncomp, num_layers = 1, dropout = 0,\n",
        "                      bidirectional = bidi, nonlinearity = 'tanh')\n",
        "    self.fc = nn.Linear(ncomp, NN2)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    y = self.rnn(x)[0]\n",
        "\n",
        "    if self.rnn.bidirectional:\n",
        "      # if the rnn is bidirectional, it concatenates the activations from the forward and backward pass\n",
        "      # we want to add them instead, so as to enforce the latents to match between the forward and backward pass\n",
        "      q = (y[:, :, :ncomp] + y[:, :, ncomp:])/2\n",
        "    else:\n",
        "      q = y\n",
        "\n",
        "    # the softplus function is just like a relu but it's smoothed out so we can't predict 0\n",
        "    # if we predict 0 and there was a spike, that's an instant Inf in the Poisson log-likelihood which leads to failure\n",
        "    z = F.softplus(self.fc(q), 10)\n",
        "\n",
        "    return z, q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAqfK3-RpPTl"
      },
      "source": [
        "Prepare input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rfgR5Exo9-O"
      },
      "source": [
        "# todo x0 should be the spikes and x1 the behaviour\n",
        "x0 = torch.from_numpy(x[:, :, :200]).to(device).float()\n",
        "x1 = torch.from_numpy(x[:, :, 200:]).to(device).float()\n",
        "\n",
        "NN1 = x1.shape[-1]\n",
        "NN2 = x0.shape[-1]\n",
        "\n",
        "# we initialize the neural network\n",
        "net = Net(ncomp, NN1, NN2, bidi = True).to(device)\n",
        "\n",
        "# special thing:  we initialize the biases of the last layer in the neural network\n",
        "# we set them as the mean firing rates of the neurons.\n",
        "# this should make the initial predictions close to the mean, because the latents don't contribute much\n",
        "net.fc.bias.data[:] = x1.mean((0,1))\n",
        "\n",
        "# we set up the optimizer. Adjust the learning rate if the training is slow or if it explodes.\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=.005)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXUOhjSlpVn_"
      },
      "source": [
        "Train network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xzV89xVpOFW"
      },
      "source": [
        "# you can keep re-running this cell if you think the cost might decrease further\n",
        "\n",
        "# we define the Poisson log-likelihood loss\n",
        "def Poisson_loss(lam, spk):\n",
        "  return lam - spk * torch.log(lam)\n",
        "\n",
        "niter = 1000\n",
        "for k in range(niter):\n",
        "  # the network outputs the single-neuron prediction and the latents\n",
        "  z, y = net(x1)\n",
        "\n",
        "  # our log-likelihood cost\n",
        "  cost = Poisson_loss(z, x0).mean()\n",
        "\n",
        "  # train the network as usual\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if k % 100 == 0:\n",
        "    print(f'iteration {k}, cost {cost.item():.4f}')\n",
        "  \n",
        "\n",
        "  rpred = z.detach().cpu().numpy()  # predicted output"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}